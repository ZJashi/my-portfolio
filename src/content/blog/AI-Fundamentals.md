---
title: 'AI Fundamentals'
date: '2026-01-23'
tags: 
  - ai
excerpt: 'Modern AI Fundamentals'
---

These are some notes that I am taking to keep up with the industry and research literature. 

# Definitions


**Autoregressive** - refers to a model or process where current 
outputs are explicitly conditioned on past outputs of the same variable or sequence.

More, precisely, a language model is **autoregressive** if it defines the joint probability of a token sequence  
$ (x_1, \dots, x_T) $ by factorizing it left-to-right using the chain rule of probability:

$$
P(x_1, \dots, x_T)
= \prod_{t=1}^{T} P(x_t \mid x_1, \dots, x_{t-1})
$$

Each token is predicted conditioned on all previously generated tokens, including those generated by the model itself.


A **Masked Autoencoder (MAE)** is a general framework (popularized in vision, but used elsewhere too):
1. Mask a large portion of the input (often 50â€“90%)
2. Encode only the visible parts
3. Use a decoder to reconstruct the missing parts
4. Train using a reconstruction loss
